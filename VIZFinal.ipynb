{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import glob\n",
    "import seaborn\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns #visualisation\n",
    "import matplotlib.pyplot as plt #visualisation\n",
    "#%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "import glob\n",
    "os.chdir('/opt/app-root/src/')\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('Data/incident*.xlsx')\n",
    "raw = pd.concat([pd.read_excel(f, sheet_name='Page 1') for f in files],ignore_index=True)\n",
    "\n",
    "df = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Parent','Customer Impact','Parent Incident','Assignment group','Assigned to','Actions taken','Activity due','Actual start','Additional assignee list','Additional comments','Approval','Approval history','Approval set','Business Contact','Business impact','Cause CI','Caused by Change','Closed by','Created by','Geography','KB Article to be created?','Location','Opened by','SLA due','Updated','Updated by','Active','Closed'], axis=1)\n",
    "\n",
    "df[\"Caller\"] = df[\"Caller\"].str.strip()\n",
    "\n",
    "DataNoevent = df[df['Caller'] != \"Event Management\"]\n",
    "DataNoevent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataNoevent\n",
    "\n",
    "DataNoevent['Caller'].unique()\n",
    "\n",
    "df_app =pd.read_csv('/opt/app-root/src/ApplicationNames.csv',encoding='ISO-8859-1')\n",
    "\n",
    "df['Short_long'] = df['Short description'].map(str) + df['Description'].map(str)\n",
    "df['Short_long'] = df['Short_long'].str.replace(\"nan\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplicationNameCI(s):\n",
    "    for x, y in zip(df_app['Application Name'], df_app['Short Name']):\n",
    "           if x in s:\n",
    "                return y\n",
    "\n",
    "df['Application'] = df['Configuration item'].apply(lambda x: ApplicationNameCI(x))\n",
    " \n",
    "df['Application'] = df['Application'].map(str)\n",
    "df['Business service'] = df['Business service'].map(str)              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app=[]\n",
    "for index, row in df.iterrows(): \n",
    "    if (row['Application']=='None'):\n",
    "        app =ApplicationNameCI(row['Business service'])\n",
    "        df['Application'][index]=app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getappname(des):\n",
    "    appname=[]\n",
    "    application_name = ['ASW2','CFMS','CLW','HLP','SBOS','RLM','BOE','PRS','CSW','UAM','APEA','ASW','Cers','EDW','BIH','WAS',\n",
    "     'WOC','PRPC','Disputes','DCS']\n",
    "#     application_name = ['asw2','cfms','clw','hlp','sbos','rlm','boe','prs','csw','uam',\n",
    "#                     'apea','asw','cers','edw','bih','woc','prpc','disputes','dcs']  \n",
    "    s1 = set(des.split())\n",
    "    s2=set(application_name)\n",
    "    appname=(s1.intersection(s2))\n",
    "    if appname:\n",
    "        return ', '.join(appname)\n",
    "    else:\n",
    "        return''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['App']=[getappname(r) for r in df['Short_long']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkappnames(df):\n",
    "    App1=df[0]\n",
    "    App2=df[1]\n",
    "    if App1 and App2:\n",
    "        if App1!=App2:\n",
    "            return App2\n",
    "        else:\n",
    "            return App2\n",
    "    elif App1:\n",
    "        return App1\n",
    "    elif App2:\n",
    "        return App2\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NewCol'] = df[['Application', 'App']].apply(checkappnames, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, column, sep='|', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split(df, 'NewCol', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(['Short_long', 'NewCol'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "list_values = []\n",
    "for i in df['Short_long']:\n",
    "    list_values.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "removed_duplicate = []\n",
    "for i in list_values:\n",
    "    #s = i\n",
    "    cleared = ' '.join(OrderedDict((w,w) for w in i.split()).keys())\n",
    "    removed_duplicate.append(cleared)\n",
    "df['without_duplicate'] = removed_duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regex = df[['without_duplicate']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex00(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ''\n",
    "        reg='STEPS TAKEN TO RESOLVE INCIDENT.*?tips.aspx|STEPS TAKEN TO RESOLVE INCIDENT.*?\\w{2}\\S\\d{4}|From:.*?Subject:|The only workaround.*?business mentioned above|Connect Issue.*?#techend|Preferred Contact No.*?#Techend|ICBOX.*?#techend|Will EU.*?Alternate|Number.*?Current Address|Contact Number.*?name number|Will EU.*?name number|Number users affected.*?AU 3008|Outage.*?contact numbers|Number.*?BANGALORE KA IN|Preferred Contact No.*?VDI connection type|Number users impacted.*?AU 3008|Number of users impacted.*?PH 1110|E.g. Mailbox Name.*?Docklands VIC 3008|Number of users affected.*?Auckland 1010|Number of users affected.*?Manyata Embassy Business Park- SEZ, BANGALORE KA IN|Number of users impacted.*?Manyata|Number of users impacted.*?BANGALORE  KA|Number of users affected.*?KA|Number of users impacted.*?NZ  1010|Number of users impacted.*?AU  /d{4}|Thanks and regards.*?V\\w{2} 3008|Report.*?tips.aspx|Number of users impacted.*?AU  3000|If issue is not resolved.*?when escalating|Number of users impacted.*?Bangalore, India|Number of users impacted.*?Address|Number of users impacted.*?CN  610041|Number of users impacted.*?AU  3121|Number of users impacted.*?AU 3008|Number of users impacted.*?PH  1110|'\n",
    "        reg+= 'Data cap.*?flowing|Splunk.*?Circum|Inbound path.*?aup4330l|Path.*?AUP4330L|path.*?PEGA_BIX|Path.*?PEGA_BIX|Preferred Contact.*?Affected|UpperGround.*?3008||When.*?recently|ANZ Support Services.*?BANGALORE KA IN'\n",
    "        regex = re.compile(reg,re.DOTALL)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex00.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex01(df):    \n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ' '\n",
    "        reg ='PRB\\d{7}|ANZ[a-zA-Z0-9]{6,}-\\d{1,}|CNWS-\\d{8}|\\W{3,}URGENT\\W{2,}|\\W{3}URGENT ASSISTANCE\\W{3}|\\w{4}\\s\\w{5}\\s\\w{3}:\\s\\w-\\d{7}|SR-\\d{7}|ANZ-AUS-\\w{3}\\W\\w{4}|\\W{3}no auto ticket\\W{3}|ANZ-SW-Work I-\\d{5,}|I-\\d{5,}|RLMAPP-\\d{6}|S-\\d{5,}|CapCisID: \\d{6,}|Lodgementid:\\d{8,}|\\W{1,}no auto ticket\\W{1,}|RLMINDX-\\d{4,}|TRF-\\d{2,}|\\w{3,}\\syou\\sin\\w{4,}'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex01.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex02(df):    \n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ' '\n",
    "        reg='App\\s\\w{2}:\\s\\d{9}|Sr-\\d{7}|Salary ID|Application\\s\\w{5}\\s\\w{2}:|\\(\\w{6}\\W\\w{7}\\W\\w{4}\\s\\w{6}\\)|\\(\\w{6}\\W\\w{7}\\W\\w{4}\\s\\w{6}\\s\\w{3}\\s\\w{7}\\)|\\(laptop/desktop/VDI/Mobile Device Type\\)|\\w{5}\\d{6}\\-\\d{5}|\\(\\w{2}\\s\\w{5}\\s\\w{3}\\)|\\w{5}\\s\\w{8}\\s\\(\\w{1}\\/\\w{1}\\)|Application\\s\\d{10}|Issue\\s\\w{11}\\:|Reported\\s\\w{5}\\s\\W|\\*no\\s\\w{4}\\s\\w{6}\\*|is\\w{4}\\:|mcp\\w{4}\\d{1}|JI\\w{2}\\s\\d{3}|SDM\\s\\d{8}|no\\s\\w{4} incident|no\\s\\w{4} incidents|aup\\d{4}\\w{1}|LAN \\w{2}\\W |Error \\w{4}age|Screen\\w{4} \\w{4}ched|Screen\\w{4} \\w{5}tory|Reference no'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        \n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ', '.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex03(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ''\n",
    "        reg = 'DPAMF\\d{6}\\-\\d{5}|TRF-\\d{2,}|\\w{3,}\\syou\\sin\\w{4,}|RLMINDX-\\d{4,}|S-\\d{2,}|SR-\\d{2,}|RLMAPP:\\s\\d{5}|APP ID: RLMAPP-/d{5}|LTD\\d{6}\\w{6}\\d{4}|LTD\\s\\d{6}\\w{6}|AO-|TSM - AO PEGA Support|ANZSDIAUSHistory\\w{5,}|'\n",
    "        reg+='Pega Id\\s\\-\\s\\w{7}|LAN ID:\\s\\w{3,8}|APP\\s\\d{10}|app\\s\\d{10}|App\\s\\d{10}|APP\\s#\\s\\d{10}|ANZRLM_CAW_HISTORY|Example :\\s\\d{8}|support group|Screenshot|Desktop Director'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex04(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text =''\n",
    "        reg = 'Workstation ID : \\w{4}-\\d{8}|Workstation ID : \\w{4}-[a-zA-Z0-9]{7,}|workstation id : \\w{4}-[a-zA-Z0-9]{7,}|Workstation : [a-zA-Z0-9]{1,}|Workstation ID : [^a-z]{14}|Workstation ID  : [^a-z]{13}|Workstation ID : [^a-z]{13}|Workstation ID : [^a-z]{12}|WORKSTATION\\s[a-zA-Z0-9]{1,}|'\n",
    "        reg+='workstation ID: \\w{12}|Workstation ID: \\w{2}-D\\w{12}|\\sWorkstation ID : \\d{8,}|Workstation ID : d{8,}|Workstation ID : NA|Workstation ID : L\\w{11,}|Workstation ID : VP\\w{12}|Workstation ID  : l\\w{12}|Workstation ID  :   \\w{4}-\\w{8}|Workstation ID  :VP\\w{12}|Workstation ID  : d\\w{12}|Workstation ID: d\\w{12}|Workstation ID  : D\\w{12}|Workstation ID  : inlt-\\w{8}'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex04.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex05(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text =''\n",
    "        reg = 'workstation id: L\\w{11,}|workstation id: D\\w{11,}||\\WWorkstation ID : \\w{1}\\/\\w{1}|\\WWorkstation ID :|Workstation ID  :'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex05.csv')\n",
    "    \n",
    "def regex06(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text =''\n",
    "        reg = '\\d{1,}\\/\\d{2}\\/\\d{2,}|\\d{1,}\\-\\d{2}\\-\\d{2,}|\\d{2}\\:\\d{2}\\:\\d{2} [ap]m|\\d{2}\\:\\d{2} [ap]m|\\d{4}\\W\\d{2}\\W\\d{2}|\\d{1,}\\w{2}\\s\\w{3,}\\s\\d{1,}'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex06.csv')\n",
    "    \n",
    "def regex07(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ''\n",
    "        reg='Â|Ã|¢|\\n|\\x82|\\x83|\\x80'\n",
    "        #'Â|Ã|'\n",
    "        #\\x82Ã\\x82Â\\x95|Ã‚Â‚ÃƒÂ‚Ã‚Â|Â\\x82Ã\\x82Â\\x94|Â\\x82Ã\\x82Â\\x93|Â\\x82Ã\\x82Â\\x92afaÂ\\x82Ã\\x82Â\\x92|Â\\x82Ã\\x82Â\\x92|Â\\x82Ã\\x82Â\\x91|Â\\x82Ã\\x82Â\\x96|Â\\x82Ã\\x82Â|Â\\x82Ã\\x82Â®|Ã\\x82Â\\x94|Â\\x82Ã\\x82Â|'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex07.csv')\n",
    "    \n",
    "def regex08(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ''\n",
    "        reg='_|sr-\\d{6,}|System Issue Description|Please|please|laptop/desktop/VDI/Mobile Device Type|Original Message|Contacts(Name and number)|Pre Post TVT Query|Salary ID|Lan ID|User best form contact|Screenshot of|Desktop Director|pegajupiter_data.anz_anzsdiaus_work_dispute|pegajupiter_data.ANZ_ANZSDCAUS_DATA_SCRPTENT|Message|Screenshot Mandatory|ANZ[a-zA-Z0-9]{6,}-\\d{1,}|Visa OnUs \\d+\\s\\d+|Visa OnUs Fraud \\d+\\s\\d+|Visa OnUs Fraud \\d+\\s\\s\\d+|\\S+@\\S+|User best \\w{4}\\s\\w{2}\\s\\w{7}\\:|Access/Application/Profile/system|Application/system|Printer/EV/Right fax|laptop/desktop/thin client|BuildingManyata|Ltd.,Eucalyptus|MAC Address|E.g. Mailbox Name/ Server details|'\n",
    "        reg+='Contacts\\(Name number\\)|Contacts\\(Name and number\\)|path\\s\\W{2}10.44.130.\\d{1,}\\Wpegamis\\WDownload|833 Collins St, Docklan VIC 3008|Workstation|Kindly|screenshot\\/attached|screenshot attached|screenshots attached|RLM ID :|RLMAPP-\\d{6}|\\w{3,}.\\w{2,}@anz.com|\\W\\w\\W\\w\\W\\W\\s\\w{1,}|STEPS TAKEN TO RESOLVE INC\\w{1,}:|System Issue Description:|Logon ID: \\w{2,}|\\w{3,}\\s\\w{7}\\W\\w{7}\\s\\w{7,}\\W\\s\\w{4}|\\w{10,}\\s\\w{2}\\s\\W\\w{6}\\W\\w{7}\\W\\w{3}\\W\\w{5,}\\s\\w{3,}\\s\\w{4}\\W|\\w{10,}\\s\\w{2}\\s|'\n",
    "        reg+='ERR-:\\s\\d{10}|ANZBAU\\d{1}\\w{5}\\d{23}|Lodgementid:\\d{8,}|\\d{1,}\\W|I-\\d{1,}|path\\s\\W{2}10.44.130.\\d{1,}\\Wpegamis\\WDownload|\\d{2}\\W\\d{2}\\W\\d{4}|nan|RLM ID :|RLMAPP-\\d{6}|\\w{3,}.\\w{2,}@anz.com|\\W\\w\\W\\w\\W\\W\\s\\w{1,}'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex08.csv')\n",
    "    \n",
    "def regex09(df):\n",
    "    short = []\n",
    "    duplicate = []\n",
    "    for index, rows in df.iterrows():\n",
    "        a = rows[0]\n",
    "        replace_text = ''\n",
    "        reg='\\w{4}PPRGMER|\\w{4}Hi|\\w{1}eam\\w{2}|\\w{1}\\d{2}ggp|Files:Offcr\\w{50,}Chnl|\\w{1}egards|\\w{1}uilding|\\w{1}anyata|'\n",
    "        reg+='\\w{1}ucalyptus|\\w{1}mbassy|\\w{1}usiness|\\w{1}ark|SEZ|KAIN|BANGALORE|BG S|S\\d{2}|ANZ\\w{3}AUS|\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*|'\n",
    "        reg+='PEGAUPMIS\\w{3,}|0uzv\\w{1,}|0u\\w{3}'\n",
    "        regex = re.compile(reg)\n",
    "        pattern = re.findall(regex, a)\n",
    "        pattern = list(set(pattern))\n",
    "        pattern = ','.join(pattern)\n",
    "        out = re.sub(regex, replace_text, a)\n",
    "        short.append(out)\n",
    "        duplicate.append(pattern)\n",
    "    df['struc_short'] = short\n",
    "    df['Features'] = duplicate\n",
    "    df.to_csv('/opt/app-root/src/Regexoutput/regex09.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex00(df_regex)\n",
    "\n",
    "df_01 = pd.read_csv('/opt/app-root/src/Regexoutput/regex00.csv',encoding='ISO-8859-1')\n",
    "df_01.drop(['without_duplicate','Features'],axis = 1, inplace = True)\n",
    "df_01 = df_01['struc_short'].astype(str)\n",
    "df_01 = df_01.to_frame()\n",
    "df_01 = df_01.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex01(df_01)\n",
    "\n",
    "df_02 = pd.read_csv('/opt/app-root/src/Regexoutput/regex01.csv',encoding='ISO-8859-1')\n",
    "df_02.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_02 = df_02['struc_short'].astype(str)\n",
    "df_02=df_02.to_frame()\n",
    "df_02=df_02.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex02(df_02)\n",
    "\n",
    "df_03 = pd.read_csv('/opt/app-root/src/Regexoutput/regex02.csv',encoding='ISO-8859-1')\n",
    "df_03.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_03 = df_03['struc_short'].astype(str)\n",
    "df_03 = df_03.to_frame()\n",
    "df_03 = df_03.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex03(df_03)\n",
    "\n",
    "df_04 = pd.read_csv('/opt/app-root/src/Regexoutput/regex03.csv',encoding='ISO-8859-1')\n",
    "df_04.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_04 = df_04['struc_short'].astype(str)\n",
    "df_04 = df_04.to_frame()\n",
    "df_04 = df_04.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex04(df_04)\n",
    "\n",
    "df_05 = pd.read_csv('/opt/app-root/src/Regexoutput/regex04.csv',encoding='ISO-8859-1')\n",
    "df_05.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_05 = df_05['struc_short'].astype(str)\n",
    "df_05 = df_05.to_frame()\n",
    "df_05 = df_05.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex05(df_05)\n",
    "\n",
    "df_06 = pd.read_csv('/opt/app-root/src/Regexoutput/regex05.csv',encoding='ISO-8859-1')\n",
    "df_06.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_06 = df_06['struc_short'].astype(str)\n",
    "df_06 = df_06.to_frame()\n",
    "df_06 = df_06.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex06(df_06)\n",
    "\n",
    "df_07 = pd.read_csv('/opt/app-root/src/Regexoutput/regex06.csv',encoding='ISO-8859-1')\n",
    "df_07.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_07 = df_07['struc_short'].astype(str)\n",
    "df_07 = df_07.to_frame()\n",
    "df_07 = df_07.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex07(df_07)\n",
    "\n",
    "df_08 = pd.read_csv('/opt/app-root/src/Regexoutput/regex07.csv',encoding='ISO-8859-1')\n",
    "df_08.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_08 = df_08['struc_short'].astype(str)\n",
    "df_08 = df_08.to_frame()\n",
    "df_08 = df_08.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "regex08(df_08)\n",
    "\n",
    "df_09 = pd.read_csv('/opt/app-root/src/Regexoutput/regex08.csv',encoding='ISO-8859-1')\n",
    "df_09.drop(['Short_long','Features'],axis = 1, inplace = True)\n",
    "df_09 = df_09['struc_short'].astype(str)\n",
    "df_09 = df_09.to_frame()\n",
    "df_09 = df_09.rename(columns = {'struc_short':'Short_long'})\n",
    "\n",
    "df_09.info()\n",
    "\n",
    "df_09['Short_long'] = df_09['Short_long'].str.lower()\n",
    "df_09['Short_long'] = df_09['Short_long'].str.replace('\\W',' ')\n",
    "\n",
    "df['struc_short']=df_09['Short_long'].values\n",
    "\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = pd.Series(\" \".join(df['struc_short']).split()).value_counts()\n",
    "print(len(word_frequency))\n",
    "##print(word_frequency)\n",
    "word_frequency = pd.Series(\" \".join(df['struc_short']).split()).value_counts()[0:70]\n",
    "word_df =pd.DataFrame({'Word':word_frequency.index, 'Freq':word_frequency.values})\n",
    " #output top 15 rows\n",
    "# word_df.head(15)\n",
    "fig = sns.set(rc = {'figure.figsize':(15,6)})\n",
    "bp = sns.barplot(x = \"Word\", y = \"Freq\", data = word_df)\n",
    "bp.set_xticklabels(bp.get_xticklabels(), rotation = 75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "add_stopwords = open(\"/opt/app-root/src/Stopwords.txt\", mode=\"r\", encoding='ISO-8859-1').read().split()\n",
    "stop_words = stop_words.union(add_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(rev):\n",
    "    rev_new = \" \".join([i for i in rev if i not in stop_words ])\n",
    "    return rev_new\n",
    "\n",
    "df['struc_short'] = [stopwords(r.split()) for r in df['struc_short']]\n",
    "\n",
    "df['struc_short'] = df['struc_short'].str.replace('\\d+','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = pd.Series(\" \".join(df['struc_short']).split()).value_counts()\n",
    "print(len(word_frequency))\n",
    "##print(word_frequency)\n",
    "word_frequency = pd.Series(\" \".join(df['struc_short']).split()).value_counts()[70:100]\n",
    "word_df =pd.DataFrame({'Word':word_frequency.index, 'Freq':word_frequency.values})\n",
    " #output top 15 rows\n",
    "# word_df.head(15)\n",
    "fig = sns.set(rc = {'figure.figsize':(15,6)})\n",
    "bp = sns.barplot(x = \"Word\", y = \"Freq\", data = word_df)\n",
    "bp.set_xticklabels(bp.get_xticklabels(), rotation = 75)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import  word_tokenize\n",
    "df['single_token'] = df.apply(lambda row:nltk.word_tokenize(row['struc_short']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0] \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_data = count_vectorizer.fit_transform(df['struc_short'])\n",
    "plot_10_most_common_words(count_data, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 4\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'wb') as f:    \n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ldavis_prepared_4'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAvis_data_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'posixpath' from '/opt/app-root/lib64/python3.6/posixpath.py'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8888/    [Ctrl-C to exit]\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.show(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
